{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4yvI-kf-myN"
      },
      "source": [
        "# Assignment 5:  MDPs and Q-learning On \"Ice\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iEuDHFs-ixs"
      },
      "source": [
        "In this project we revisit Markov Decision Processes while also trying out Q-Learning, the reinforcement learning approach that associates utilities with attempting actions in states.\n",
        "The problem that we’re attempting to solve is the following variation on the “Wumpus World” sometimes used as an example in lecture and the textbook.\n",
        "\n",
        "1.  There is a grid of spaces in a rectangle.  Each space can contain a pit (negative reward), gold (positive reward), or nothing.\n",
        "2.  The rectangle is effectively surrounded by walls, so anything that would move you outside the rectangle, instead moves you to the edge of the rectangle.\n",
        "3.  The floor is icy.  Any attempt to move in a cardinal direction results in moving a somewhat random number of spaces in that direction.  The exact probabilities of moving each number of spaces are given in the problem description.  (If you slide too far, see rule #2.)\n",
        "4.  Landing on a pit or gold effectively “ends the run,” for both a Q learner and an agent later trying out the policy.  It’s game over.  (To simulate this during Q learning, set all Q values for the space to equal its reward, then start over from a random space.)  Note that it's still possible to slide past a pit or gold - this doesn't end the run.\n",
        "\n",
        "A sample input looks like this:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_PSiQiO-_2y"
      },
      "source": [
        "sampleMDP = \"\"\"0.7 0.2 0.1\n",
        "- - P - -\n",
        "- - G P -\n",
        "- - P - -\n",
        "- - - - -\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CmLASMB_Gsd"
      },
      "source": [
        "\n",
        "The first line says that the probabilities of moving one, two, or three spaces in the direction of movement are 0.6, 0.3, and 0.1.   The rest is a map of the environment, where a dash is an empty space, P is a pit, and G is gold.\n",
        "\n",
        "1.  mdp_solve() uses value iteration and the Bellman equation.  ITERATIONS refers to the number of complete passes you perform over all states.  I initialize the utilities to the rewards of each state.  We don’t update the rewards spaces from their initial rewards; since they end the trial, they have no future utility.  We also don't update utilities in-place as we iterate through them, but create a fresh array of utilities with each pass, in order to avoid biasing moves in the directions that have already been updated.\n",
        "\n",
        "2.  q_solve() runs ITERATIONS trials in which a learner starts in a random empty square and moves until it hits a pit or gold, in which case, the trial is over.  (If it was randomly dropped into gold or a pit, the trial is immediately over.)  The learner moves by deciding randomly whether to choose a random direction (with probability EXPLORE_PROB) or move according to the best Q-value of its current square (otherwise).  We simulate the results of the move on slippery ice to determine where the learner ended up - then apply the Q-learning equation given in lecture and the textbook. \n",
        "\n",
        "The fact that a trial ends immediately on finding gold or a pit means that we want to handle those spaces in a special way.  Normally Q values are updated on moving to the next state, but we won’t see any next state in these cases.  So, to handle this, when the agent discovers one of these rewards, we set all the Q values for that space to the associated reward before quitting the trial.  So, for example, if gold is worth 100 and it’s discovered in square x, Q(x,UP) = 100, Q(x,RIGHT) = 100, Q(x, DOWN) = 100, and Q(x, LEFT) = 100.  There’s no need to apply the rest of the Q update equation when the trial is ending, because that’s all about future rewards, and there’s no future when the trial is ending.  But now the spaces that can reach that space will evaluate themselves appropriately.  (Before being \"discovered,\" the square should have no utility.)\n",
        "\n",
        "We use the GOLD_REWARD, PIT_REWARD, LEARNING_RATE, and DISCOUNT_FACTOR constants at the top of the file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZW7aHFXpUQ9l"
      },
      "source": [
        "\"\"\" \"MDPs on Ice - Assignment 5\"\"\"\n",
        "\n",
        "import sys\n",
        "import numpy\n",
        "import random\n",
        "import copy\n",
        "\n",
        "GOLD_REWARD = 250.0\n",
        "PIT_REWARD = -150.0\n",
        "DISCOUNT_FACTOR = 0.8\n",
        "EXPLORE_PROB = 0.2 # for Q-learning\n",
        "LEARNING_RATE = 0.01\n",
        "ITERATIONS = 100000\n",
        "MAX_MOVES = 1000\n",
        "MIN = -1000\n",
        "ACTIONS = 4\n",
        "UP = 0\n",
        "RIGHT = 1\n",
        "DOWN = 2\n",
        "LEFT = 3\n",
        "MOVES = ['U', 'R', 'D', 'L']\n",
        "\n",
        "# Fixed random number generator seed for result reproducibility --\n",
        "# don't use a random number generator besides this to match sol\n",
        "random.seed(5100)\n",
        "\n",
        "class Problem:\n",
        "    \"\"\"Represents the physical space, transition probabilities, reward locations, and approach\n",
        "\n",
        "    ...in short, the info in the text file\n",
        "\n",
        "    Attributes:\n",
        "        move_probs (List[float]):  probabilities of going 1,2,3 spaces\n",
        "        map (List[List(string)]]:  \"-\" (safe, empty space), \"G\" (gold), \"P\" (pit)\n",
        "\n",
        "    String format consumed looks like\n",
        "    0.7 0.2 0.1   [probability of going 1, 2, 3 spaces]\n",
        "    - - - - - - P - - - -   [space-delimited map rows]\n",
        "    - - G - - - - - P - -   [G is gold, P is pit]\n",
        "\n",
        "    You can assume the maps are rectangular, although this isn't enforced\n",
        "    by this constructor.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, probstring):\n",
        "        \"\"\" Consume string formatted as above\"\"\"\n",
        "        self.map = []\n",
        "        for i, line in enumerate(probstring.splitlines()):\n",
        "            if i == 0:\n",
        "                self.move_probs = [float(s) for s in line.split()]\n",
        "            else:\n",
        "                self.map.append(line.split())\n",
        "\n",
        "    def solve(self, iterations, use_q):\n",
        "        \"\"\" Wrapper for MDP and Q solvers.\n",
        "\n",
        "        Args:\n",
        "            iterations (int):  Number of iterations (but these work differently for the two solvers)\n",
        "            use_q (bool):  False means use MDP value iteration, true means use Q-learning\n",
        "        Returns:\n",
        "            A Policy, in either case (what to do in each square; see class below)\n",
        "        \"\"\"\n",
        "\n",
        "        if use_q:\n",
        "            return q_solve(self, iterations)\n",
        "        return mdp_solve(self, iterations)\n",
        "\n",
        "class Policy:\n",
        "    \"\"\" Abstraction on the best action to perform in each state.\n",
        "\n",
        "    This is a string list-of-lists map similar to the problem input, but a character gives the best\n",
        "    action to take in each non-reward square (see MOVES constant at top of file).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, problem):\n",
        "        \"\"\"Args:\n",
        "\n",
        "        problem (Problem):  The MDP problem this is a policy for\n",
        "        \"\"\"\n",
        "        self.best_actions = copy.deepcopy(problem.map)\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"Join the characters in the policy into one big space-separated, multline string\"\"\"\n",
        "        return '\\n{}\\n'.format('\\n'.join([' '.join(row) for row in self.best_actions]))\n",
        "\n",
        "def roll_steps(move_probs, row, col, move, rows, cols):\n",
        "    \"\"\"Calculates the new coordinates that result from a move.\n",
        "\n",
        "    Includes the \"roll of the dice\" for transition probabilities and checking arena boundaries.\n",
        "\n",
        "    Helper for try_policy and q_solve - probably useful in your Q-learning implementation.\n",
        "\n",
        "    Args:\n",
        "        move_probs (List[float]):  Transition probabilities for the ice (from problem)\n",
        "        row, col (int, int):  location of agent before moving\n",
        "        move (string):  The direction of move as a MOVES character (not an int constant!)\n",
        "        rows, cols (int, int):  number of rows and columns in the map\n",
        "\n",
        "    Returns:\n",
        "        new_row, new_col (int, int):  The new row and column after moving\n",
        "    \"\"\"\n",
        "    displacement = 1\n",
        "    total_prob = 0\n",
        "    move_sample = random.random()\n",
        "    for p, prob in enumerate(move_probs):\n",
        "        total_prob += prob\n",
        "        if move_sample <= total_prob:\n",
        "            displacement = p+1\n",
        "            break\n",
        "    # Handle \"slipping\" into edge of map\n",
        "    new_row = row\n",
        "    new_col = col\n",
        "    if not isinstance(move, str):\n",
        "        print(\"Warning: roll_steps wants str for move, got a different type\")\n",
        "    if move == \"U\":\n",
        "        new_row -= displacement\n",
        "        if new_row < 0:\n",
        "            new_row = 0\n",
        "    elif move == \"R\":\n",
        "        new_col += displacement\n",
        "        if new_col >= cols:\n",
        "            new_col = cols-1\n",
        "    elif move == \"D\":\n",
        "        new_row += displacement\n",
        "        if new_row >= rows:\n",
        "            new_row = rows-1\n",
        "    elif move == \"L\":\n",
        "        new_col -= displacement\n",
        "        if new_col < 0:\n",
        "            new_col = 0\n",
        "    return new_row, new_col\n",
        "\n",
        "\n",
        "def try_policy(policy, problem, iterations):\n",
        "    \"\"\"Returns average utility per move of the policy.\n",
        "\n",
        "    Average utility is as measured by \"iterations\" random drops of an agent onto empty\n",
        "    spaces, running until gold, pit, or time limit MAX_MOVES is reached\n",
        "\n",
        "    Args:\n",
        "        policy (Policy):  the policy the agent is following\n",
        "        problem (Problem):  the environment description\n",
        "        iterations (int):  the number of random trials to run\n",
        "    \"\"\"\n",
        "    total_utility = 0\n",
        "    total_moves = 0\n",
        "    for _ in range(iterations):\n",
        "        # Resample until we have an empty starting square\n",
        "        while True:\n",
        "            row = random.randrange(0, len(problem.map))\n",
        "            col = random.randrange(0, len(problem.map[0]))\n",
        "            if problem.map[row][col] == \"-\":\n",
        "                break\n",
        "        for moves in range(MAX_MOVES):\n",
        "            total_moves += 1\n",
        "            policy_rec = policy.best_actions[row][col]\n",
        "            # Take the move - roll to see how far we go, bump into map edges as necessary\n",
        "            row, col = roll_steps(problem.move_probs, row, col, policy_rec, \\\n",
        "                                  len(problem.map), len(problem.map[0]))\n",
        "            if problem.map[row][col] == \"G\":\n",
        "                total_utility += GOLD_REWARD\n",
        "                break\n",
        "            if problem.map[row][col] == \"P\":\n",
        "                total_utility += PIT_REWARD\n",
        "                break\n",
        "    return total_utility / total_moves\n",
        "\n",
        "def validate(cord, rows, cols):\n",
        "  row = cord[0]\n",
        "  col = cord[1]\n",
        "\n",
        "  if row < 0:\n",
        "    row = 0\n",
        "  elif row >= rows:\n",
        "    row = rows - 1\n",
        "  \n",
        "  if col < 0:\n",
        "    col = 0\n",
        "  elif col >= cols:\n",
        "    col = cols - 1\n",
        "  \n",
        "  return (row, col)\n",
        "\n",
        "def mdp_solve(problem, iterations):\n",
        "    \"\"\" Perform value iteration for the given number of iterations on the MDP problem.\n",
        "\n",
        "    Here, the squares with rewards can be initialized to the reward values, since value iteration\n",
        "    assumes complete knowledge of the environment and its rewards.\n",
        "\n",
        "    Args:\n",
        "        problem (Problem):  description of the environment\n",
        "        iterations (int):  number of complete passes over the utilities\n",
        "    Returns:\n",
        "        a Policy (though you may design this to return utilities as a second return value)\n",
        "    \"\"\"\n",
        "    # Initialize some variables.\n",
        "    policy = Policy(problem)\n",
        "    rows = len(problem.map)\n",
        "    cols = len(problem.map[1])\n",
        "    prev = []\n",
        "\n",
        "    # Iterate through the rows.\n",
        "    for row in range(rows):\n",
        "      prev.append([])\n",
        "      # Iterate through the columns.\n",
        "      for col in range(cols):\n",
        "        # Update the board state based on what we know.\n",
        "        if problem.map[row][col] == '-':\n",
        "          prev[row].append(0)\n",
        "        elif problem.map[row][col] == 'P':\n",
        "          prev[row].append(PIT_REWARD)\n",
        "        elif problem.map[row][col] == 'G':\n",
        "          prev[row].append(GOLD_REWARD)\n",
        "\n",
        "    # Iterate for number of iterations specified.\n",
        "    for ii in range(iterations):\n",
        "      utils = []\n",
        "      # Iterate through the rows.\n",
        "      for row in range(rows):\n",
        "        utils.append([])\n",
        "        # Iterate through the columns.\n",
        "        for col in range(cols):\n",
        "          if problem.map[row][col] != '=':\n",
        "            utils[row].append(prev[row][col])\n",
        "            continue\n",
        "          \n",
        "          max_utility = -1000\n",
        "          \n",
        "          # Simulate the specified moves.\n",
        "          for move in MOVES:\n",
        "            new_coords = []\n",
        "            for x in range(len(problem.move_probs)):\n",
        "              new_coords.append([row, col])\n",
        "\n",
        "            # Handle the designated current move direction.\n",
        "            if move == 'U':\n",
        "              for x in range(len(new_coords)):\n",
        "                new_coords[x][0] -= (x + 1)\n",
        "            elif move == \"R\":\n",
        "              for x in range(len(new_coords)):\n",
        "                new_coords[x][1] += (x + 1)\n",
        "            elif move == \"D\":\n",
        "              for x in range(len(new_coords)):\n",
        "                new_coords[x][0] += (x + 1)\n",
        "            elif move == \"L\":\n",
        "              for x in range(len(new_coords)):\n",
        "                new_coords[x][1] -= (x + 1)\n",
        "\n",
        "            # Do the math.\n",
        "            for x in range(len(new_coords)):\n",
        "              new_coords[x] = list(get_validated_coord(tuple(new_coords[x]), num_rows, num_cols))\n",
        "            utility = 0\n",
        "            for x in range(len(problem.move_probs)):\n",
        "              utility += problem.move_probs[x] * prev[new_coords[x][0]][new_coords[x][1]]\n",
        "            utility = utility * DISCOUNT_FACTOR\n",
        "            \n",
        "            # Make the move if it is right.\n",
        "            if utility > max_utility:\n",
        "              max_utility = utility\n",
        "              policy.best_actions[row][col] = move \n",
        "          utils[row].append(max_utility)\n",
        "      \n",
        "      # Update for next iteration.\n",
        "      prev = utils  \n",
        "    \n",
        "    return policy\n",
        "\n",
        "def q_solve(problem, iterations):\n",
        "    \"\"\"q_solve:  Use Q-learning to find a good policy on an MDP problem.\n",
        "\n",
        "    Each iteration corresponds to a random drop of the agent onto the map, followed by moving\n",
        "    the agent until a reward is reached or MAX_MOVES moves have been made.  When an agent\n",
        "    is sitting on a reward, update the utility of each move from the space to the reward value\n",
        "    and end the iteration.  (For simplicity, the agent also does this if just dropped there.)\n",
        "    The agent does not \"know\" reward locations in its Q-values before encountering the space\n",
        "    and \"discovering\" the reward.\n",
        "\n",
        "    Note that texts differ on when to pay attention to this reward - this code follows the\n",
        "    convention of scoring rewards of the space you are moving *from*, plus discounted best q-value\n",
        "    of where you landed.\n",
        "\n",
        "    Assume epsilon-greedy exploration.  Leave reward letters as-is in the policy,\n",
        "    to make it more readable.  You'll probably want a helper for the q-update,\n",
        "    to test independently.\n",
        "\n",
        "    Args:\n",
        "        problem (Problem):  The environment\n",
        "        iterations (int):  The number of runs from random start to reward encounter\n",
        "    Returns:\n",
        "        A Policy for the map\n",
        "    \"\"\"\n",
        "    # Initialize needed variables.\n",
        "    policy = Policy(problem)\n",
        "    rows = len(problem.map)\n",
        "    cols = len(problem.map[1])\n",
        "    qvals = []\n",
        "\n",
        "    # Iterate through the rows.\n",
        "    for row in range(rows):\n",
        "      qvals.append([])\n",
        "      # Iterate through the columns.\n",
        "      for col in range(cols):\n",
        "        # Update the board state based on what we know.\n",
        "        if problem.map[row][col] == '-':\n",
        "          qvals[row].append({'U': 0,'R': 0,'D': 0,'L': 0})\n",
        "        elif problem.map[row][col] == 'P':\n",
        "          qvals[row].append({'U': PIT_REWARD,'R': PIT_REWARD,'D': PIT_REWARD,'L': PIT_REWARD})\n",
        "        elif problem.map[row][col] == 'G':\n",
        "          qvals[row].append({'U': GOLD_REWARD,'R': GOLD_REWARD,'D': GOLD_REWARD,'L': GOLD_REWARD})\n",
        "    \n",
        "    # Iterate throught he designated iteration amount.\n",
        "    for ii in range(iterations):\n",
        "      s = (random.randint(0, rows - 1), random.randint(0, cols - 1))\n",
        "      if problem.map[s[0]][s[1]] != '-':\n",
        "        continue\n",
        "      \n",
        "      moves = 0\n",
        "      \n",
        "      # Go until we land on a game ending square or hit the  max moves.\n",
        "      while problem.map[s[0]][s[1]] == '-' and moves < MAX_MOVES:\n",
        "        action = 'U'\n",
        "        row = s[0]\n",
        "        col = s[1]\n",
        "        maxq = qvals[row][col]['U']\n",
        "        for move in MOVES:\n",
        "          if qvals[row][col][move] > maxq:\n",
        "            action = move\n",
        "            maxq = qvals[row][col][move]\n",
        "        if random.random() < EXPLORE_PROB:\n",
        "          action = MOVES[random.randint(0, 3)]\n",
        "        \n",
        "        # Updates.\n",
        "        next_row, next_col = roll_steps(problem.move_probs, row, col, action, rows, cols)\n",
        "        next_s = (next_row, next_col)\n",
        "        nmq = MIN\n",
        "        \n",
        "        for move in MOVES:\n",
        "          if qvals[next_row][next_col][move] > nmq:\n",
        "            nmq = qvals[next_row][next_col][move]\n",
        "        \n",
        "        qvals[row][col][action] += LEARNING_RATE * (DISCOUNT_FACTOR * nmq - qvals[row][col][action])\n",
        "        maxq = MIN\n",
        "        \n",
        "        for move in MOVES:\n",
        "          if qvals[row][col][move] > maxq:\n",
        "            maxq = qvals[row][col][move]\n",
        "            policy.best_actions[row][col] = move\n",
        "        \n",
        "        s = (next_row, next_col)\n",
        "        moves += 1\n",
        "    \n",
        "    return policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cuaEcLvAoYK"
      },
      "source": [
        "deterministic_test = \"\"\"1.0\n",
        "- - P - -\n",
        "- - G P -\n",
        "- - P - -\n",
        "- - - - -\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ABkxRiTA4Wi"
      },
      "source": [
        "# Notice that we counterintuitively are most likely to go 2 spaces here\n",
        "very_slippy_test = \"\"\"0.2 0.7 0.1\n",
        "- - P - -\n",
        "- - G P -\n",
        "- - P - -\n",
        "- - - - -\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lqg8ZZUCBYYl"
      },
      "source": [
        "big_test = \"\"\"0.6 0.3 0.1\n",
        "- P - G - P - - G -\n",
        "P G - P - - - P - -\n",
        "P P - P P - P - P -\n",
        "P - - P P - - - - P\n",
        "- - - - - - - - P G\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHZ99I9uBmiH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3788ee2e-dc3c-4f7f-e6bc-a29c89174573"
      },
      "source": [
        "# MDP value iteration tests\n",
        "print(Problem(deterministic_test).solve(ITERATIONS, False))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "- - P - -\n",
            "- - G P -\n",
            "- - P - -\n",
            "- - - - -\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txLGS4pUwhh7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "395093d6-d4ff-4ea3-abcb-af63b51ec093"
      },
      "source": [
        "print(Problem(sampleMDP).solve(ITERATIONS, False))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "- - P - -\n",
            "- - G P -\n",
            "- - P - -\n",
            "- - - - -\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnprAX2uwiDI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aa6477d-3723-4477-d385-04050eaa1210"
      },
      "source": [
        "print(Problem(very_slippy_test).solve(ITERATIONS, False))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "- - P - -\n",
            "- - G P -\n",
            "- - P - -\n",
            "- - - - -\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INhKxA6twic8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85e97a12-2bb6-4205-a1de-fe7e6698d33a"
      },
      "source": [
        "print(Problem(big_test).solve(ITERATIONS, False))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "- P - G - P - - G -\n",
            "P G - P - - - P - -\n",
            "P P - P P - P - P -\n",
            "P - - P P - - - - P\n",
            "- - - - - - - - P G\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfUJKMPtCRCs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "150bcccb-7b90-4ca6-bbb4-401576fb695c"
      },
      "source": [
        "# Q-learning tests\n",
        "# Set seed every time for consistent executions;\n",
        "# comment out to get different random runs\n",
        "random.seed(5100)\n",
        "print(Problem(deterministic_test).solve(ITERATIONS, True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "R D P R D\n",
            "R R G P D\n",
            "R U P D L\n",
            "R U L L L\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08cHCoI6wqak",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bed93cfa-9fff-453a-be54-27f9b496a2b9"
      },
      "source": [
        "random.seed(5100)\n",
        "print(Problem(sampleMDP).solve(ITERATIONS, True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "D D P R D\n",
            "R R G P D\n",
            "U U P D D\n",
            "U U L L L\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMM3kelxwqsx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbabe2e2-7009-498a-beef-67aa4475ab8a"
      },
      "source": [
        "random.seed(5100)\n",
        "print(Problem(very_slippy_test).solve(ITERATIONS, True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "D L P R D\n",
            "R L G P L\n",
            "D D P R D\n",
            "U L U R U\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWu_w30AwrP9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4358be04-8083-4a83-c680-5ae2d6bc31f3"
      },
      "source": [
        "random.seed(5100)\n",
        "print(Problem(big_test).solve(ITERATIONS, True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "U P R G L P R R G L\n",
            "P G U P U R U P U U\n",
            "P P U P P D P D P U\n",
            "P D U P P D D D L P\n",
            "R R U L L L L L P G\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbP5VrvF9dIt"
      },
      "source": [
        "Here are a few thought questions:\n",
        "\n",
        "**1) In an earlier version of this code, students sometimes found that some bad luck with pits near the end of the Q-learning training could cause square (0,1) of sample MDP to believe that going right was a bad idea.  What parameter would be best to adjust in order to mitigate the effect of bad luck near the end of a run?  In what direction should this parameter be adjusted?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzD5X8wY-5gS"
      },
      "source": [
        "Because the pit is right behind where the gold is, we need to put priority on going right in this situation because we will hit the gold before the pit, causing the end of the game. We therefore need to change the problem state parameter to describe the probability and reward of going right differently. Keep 1 square the same, but since the gold is 2 away, we can group 2 and 3's probabilities with 2's rewards because it could never get to three spaces away (the pit)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CkLwZnX-87e"
      },
      "source": [
        "**2) The value iteration MDP solver updates all squares an equal number of times.  The Q-learner does not.  Which squares might we expect the Q-learner to update the most?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEhHogqO_7fC"
      },
      "source": [
        "Because the Q-learner only updates when it is sitting on a reward, and it moreover only updates the squares/moves that led to a successful procurement of the reward, we can assume that the squares that are going to get updated the most are the ones with a clear straight shot to gold. If the start is placed random, and all further moves are also random, then the farther away we start the less likely we are to acheive, so those squares will get updated much less. square that have a ~1/4 chance of ending with a reward will therefore have a great chance of getting updated, and squares that are able to reach those squares will be the next most likely to be updated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeGcHwKR_-m7"
      },
      "source": [
        "**3) Suppose we change the state information so that, instead of knowing its coordinates on the map, the agent instead knows just the locations of all rewards in a 5x5 square with the agent at the square center.  Thus, at the start of every run, it may not know exactly where it is, but it knows what is in the vicinity.  It also does not know the transition model.  (Suppose also that the map itself is very big, making it potentially worthwhile for an agent to \"realize where it is\" in the map when it isn't near gold.)**\n",
        "\n",
        "**a) We can't use our value iteration here.  Why?**\n",
        "\n",
        "**b) How many states are possible, assuming the contents of the agent's own square don't matter?  Is a lookup table of Q-values feasible?  (Let's define \"feasible\" as \"able to be stored in a gig or less of memory,\" assuming 64-bit values.)**\n",
        "\n",
        "**c) Suppose we were to use the Widrow-Hoff rule to generalize over these states instead, using the 48 boolean features of \"pit at (row,col)\" and \"gold at (row,col)\".  (The coordinates here refer to the 5x5 square.)  This is obviously less expensive, memory-wise, but what's another advantage over using a lookup table for the Q-values?  What's a disadvantage?**\n",
        "\n",
        "**d) Now consider a reasonable neural network applied to this problem (details left up to you), compared to Widrow-Hoff.  What's an advantage?  What's a disadvantage?**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Inb8brIUIk8U"
      },
      "source": [
        "a.) If we never know exactly where we are, but we know where nearby gold is, we will never really be able to correlate our moves to success. The point of iterations is not ONLY to give us the gold, but also to avoid pits, and if we are solely trying to go for gold, we will have incosistnent success each time because the pits will be different based on where we are in the map. Since we can't track our coordinates, we can't say where we are for certain.\n",
        "\n",
        "b.) We would have a grid of 5x5 squares minus the square we are currently standing on, so 24. Each square can have 3 possible values, and those values have no effect on what the value is next to them. Therefore we have 3^24 possible game states to keep track of, or roughly 280 billion possible states. This is not feasible both due to time and storage. \n",
        "\n",
        "c.) This style is obviously much tamer memory-wise, but it's also faster. Although lookup tables are constant time complexity, we see by the answer to part c that it would take a decent amount of time to find each state. With Widrow-Hoff rules, we will be able to generalize and provide weights to only the options available to us in the given state, thus minimizing the time it take to lookup and giving us a good shot at making a good decision. This would also be a much faster way of training. One downside to this method is that it will not give as accurate of results as the previous solution, becuase it is using a best guess whereas the previous solution was literally looking up the exact solution. \n",
        "\n",
        "d.) One advantage of using a neural network is that after training on many different games (both successes and failures) our neural network (assuming it does its job) will draw positive and negative correlations to the given game states that may not be able to be represented by the simple weight system that Widrow-Hoff uses or apparent to a human. A major downside is that you have to be very careful what you traing the neural network on becuase if it attaches to a correlation that is not representative of the problem space as a whole, then it will start inaccurately assessing future situations and making poor decisions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78PjFoZBOLfp"
      },
      "source": [
        ""
      ]
    }
  ]
}